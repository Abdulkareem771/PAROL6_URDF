{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResUNet Weld Seam Segmentation Training Pipeline\n",
    "This notebook handles:\n",
    "1. Downloading the `u-net_model` v4 dataset from Roboflow.\n",
    "2. Defining the pure Deep Residual UNet (ResUNet) architecture.\n",
    "3. Setting up data loaders with Image Augmentation (Albumentations).\n",
    "4. Training the model with Mixed Precision, BCE+Dice Loss, and Cosine Annealing.\n",
    "5. Running predictions on the test set and exporting `best_resunet_seam.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q roboflow albumentations\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from roboflow import Roboflow\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the API key from your workspace\n",
    "rf = Roboflow(api_key=\"rf_HrQ6aUiVG3PmJKFOe8pmmXpxol62\")\n",
    "project = rf.workspace(\"computer-vision-yyh42\").project(\"u-net_model\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"png-mask-semantic\")\n",
    "\n",
    "DATA_YAML_PATH = dataset.location\n",
    "print(\"Dataset downloaded to:\", DATA_YAML_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Residual UNet (ResUNet) Architecture\n",
    "Instead of plain convolutions, ResUNet uses residual blocks. This allows gradients to flow smoothly, preventing vanishing gradients, which is critical for thin paths like 1-pixel weld seams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_c)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_c != out_c:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + res\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=1):\n",
    "        super().__init__()\n",
    "        # Initial layer\n",
    "        self.conv_init = nn.Sequential(\n",
    "            nn.Conv2d(in_c, 64, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1, bias=False)\n",
    "        )\n",
    "        self.shortcut_init = nn.Sequential(\n",
    "            nn.Conv2d(in_c, 64, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        # Encoders\n",
    "        self.res1 = ResidualBlock(64, 128, stride=2)\n",
    "        self.res2 = ResidualBlock(128, 256, stride=2)\n",
    "        self.res3 = ResidualBlock(256, 512, stride=2)\n",
    "        \n",
    "        # Decoders\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        self.dec3 = ResidualBlock(512, 256) \n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.dec2 = ResidualBlock(256, 128)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.dec1 = ResidualBlock(128, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, out_c, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial block with identity adding\n",
    "        res_init = self.shortcut_init(x)\n",
    "        c1 = self.conv_init(x) + res_init\n",
    "        \n",
    "        c2 = self.res1(c1)\n",
    "        c3 = self.res2(c2)\n",
    "        c4 = self.res3(c3)\n",
    "        \n",
    "        # Decode\n",
    "        u3 = self.up3(c4)\n",
    "        u3 = torch.cat([u3, c3], dim=1)\n",
    "        d3 = self.dec3(u3)\n",
    "        \n",
    "        u2 = self.up2(d3)\n",
    "        u2 = torch.cat([u2, c2], dim=1)\n",
    "        d2 = self.dec2(u2)\n",
    "        \n",
    "        u1 = self.up1(d2)\n",
    "        u1 = torch.cat([u1, c1], dim=1)\n",
    "        d1 = self.dec1(u1)\n",
    "        \n",
    "        out = self.final_conv(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeldSeamDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = image_dir.replace('images', 'masks')\n",
    "        if not os.path.exists(self.mask_dir):\n",
    "            # Sometimes roboflow puts them in different places\n",
    "            pass\n",
    "        self.images = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        # Find corresponding mask\n",
    "        mask_candidates = [f for f in os.listdir(self.mask_dir) if f.startswith(base_name)]\n",
    "        if len(mask_candidates) == 0:\n",
    "            raise FileNotFoundError(f\"No mask found for {img_name} in {self.mask_dir}\")\n",
    "        mask_path = os.path.join(self.mask_dir, mask_candidates[0])\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # Binary mask\n",
    "        mask = (mask > 0).astype(np.float32)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            \n",
    "        return image, mask.unsqueeze(0)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, p=0.3),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "try:\n",
    "    train_dataset = WeldSeamDataset(os.path.join(DATA_YAML_PATH, 'train', 'images'), transform=train_transform)\n",
    "    val_dataset = WeldSeamDataset(os.path.join(DATA_YAML_PATH, 'valid', 'images'), transform=val_transform)\n",
    "except Exception as e:\n",
    "    print(\"Error loading datasets:\", e)\n",
    "    train_dataset, val_dataset = [], []\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2) if len(train_dataset)>0 else []\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2) if len(val_dataset)>0 else []\n",
    "\n",
    "print(f\"Train images: {len(train_dataset)}, Val images: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Function\n",
    "We use BCE + Dice Loss. Dice Loss specifically helps segment thin lines gracefully, since it scores intersection over union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, logits, targets, smooth=1.0):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        \n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs_flat = probs.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        intersection = (probs_flat * targets_flat).sum()\n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (probs_flat.sum() + targets_flat.sum() + smooth)\n",
    "        \n",
    "        return bce_loss + dice_loss\n",
    "\n",
    "def compute_dice_coeff(logits, targets):\n",
    "    probs = torch.sigmoid(logits) > 0.5\n",
    "    intersection = (probs & (targets > 0.5)).sum().float()\n",
    "    return (2. * intersection) / (probs.sum() + targets.sum() + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "Trains the ResUNet model using Mixed Precision for speed and reduced memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet(in_c=3, out_c=1).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "criterion = DiceBCELoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_dice = 0.0\n",
    "\n",
    "train_losses, val_losses, val_dices = [], [], []\n",
    "\n",
    "if len(train_loader) > 0:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for imgs, masks in train_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, masks)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_dice = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    preds = model(imgs)\n",
    "                    loss = criterion(preds, masks)\n",
    "                    val_loss += loss.item()\n",
    "                    val_dice += compute_dice_coeff(preds, masks).item()\n",
    "                    \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_dice = val_dice / len(val_loader)\n",
    "        val_dices.append(val_dice)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f} | Val Dice: {val_dice:.4f}\")\n",
    "        \n",
    "        if val_dice > best_val_dice:\n",
    "            best_val_dice = val_dice\n",
    "            torch.save(model.state_dict(), \"best_resunet_seam.pth\")\n",
    "            print(\"--> Saved new best model\")\n",
    "\n",
    "    print(\"Training Complete. Best Val Dice:\", best_val_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plotting Results and Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_losses) > 0:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curves')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_dices, label='Val Dice Coeff')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Accuracy (Dice)')\n",
    "    plt.show()\n",
    "\n",
    "# To download the weights locally:\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('best_resunet_seam.pth')\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}