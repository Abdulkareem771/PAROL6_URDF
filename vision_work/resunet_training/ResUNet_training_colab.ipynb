{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbpf0f4NdtAU"
      },
      "source": [
        "# ResUNet Weld Seam Segmentation Training Pipeline\n",
        "This notebook handles:\n",
        "1. Downloading the `u-net_model` v4 dataset from Roboflow.\n",
        "2. Defining the pure Deep Residual UNet (ResUNet) architecture.\n",
        "3. Setting up data loaders with Image Augmentation (Albumentations).\n",
        "4. Training the model with Mixed Precision, BCE+Dice Loss, and Cosine Annealing.\n",
        "5. Running predictions on the test set and exporting `best_resunet_seam.pth`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-qc0ZZ7dtAV",
        "outputId": "cfdeac09-9aa8-4248-d515-49e0f32d22d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/94.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install -q roboflow albumentations\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from roboflow import Roboflow\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpDbR8zYdtAW"
      },
      "source": [
        "## 1. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "\n",
        "# Zip is already in session storage at /content/\n",
        "zip_path = \"/content/U-Net_model.v4i.png-mask-semantic.zip\"\n",
        "\n",
        "# Extract it\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(\"/content/dataset\")\n",
        "\n",
        "# Show what we got\n",
        "for root, dirs, files_list in os.walk(\"/content/dataset\"):\n",
        "    for f in files_list[:5]:\n",
        "        print(os.path.join(root, f))\n",
        "    break\n",
        "\n",
        "DATA_YAML_PATH = \"/content/dataset\"\n",
        "print(\"\\nDataset ready at:\", DATA_YAML_PATH)"
      ],
      "metadata": {
        "id": "3JOe51fekEeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a3b0ea-2c70-48c0-e667-502754f7aa82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset/README.roboflow.txt\n",
            "/content/dataset/README.dataset.txt\n",
            "\n",
            "Dataset ready at: /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "\n",
        "zip_path = \"/content/U-Net_model.v4i.png-mask-semantic.zip\"\n",
        "\n",
        "# Clean up any previous attempts\n",
        "!rm -rf /content/dataset\n",
        "os.makedirs(\"/content/dataset\", exist_ok=True)\n",
        "\n",
        "# Extract\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(\"/content/dataset\")\n",
        "\n",
        "# Print the folder structure deeply\n",
        "print(\"Dataset Directory Structure:\")\n",
        "for root, dirs, files_list in os.walk(\"/content/dataset\"):\n",
        "    level = root.replace(\"/content/dataset\", \"\").count(os.sep)\n",
        "    indent = \" \" * 4 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    if level < 2:  # Print a couple of files to verify\n",
        "        subindent = \" \" * 4 * (level + 1)\n",
        "        for f in files_list[:2]:\n",
        "            print(f\"{subindent}{f}\")\n",
        "\n",
        "DATA_YAML_PATH = \"/content/dataset\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-GT0NLlm53A",
        "outputId": "cc590908-e35e-4f5d-dd8e-33ccd3e39fcb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Directory Structure:\n",
            "dataset/\n",
            "    README.roboflow.txt\n",
            "    README.dataset.txt\n",
            "    train/\n",
            "        12_jpg.rf.d76e19964deace7e928253e04407e333.jpg\n",
            "        53_jpg.rf.2edb65eedb0e09e9820d04c2af88ba61_mask.png\n",
            "    valid/\n",
            "        N_32_jpg.rf.930edbe76382470c7bc3e71cfa9cb103_mask.png\n",
            "        41_jpg.rf.d8e1bdafa108596afc14bc3289fcb65f.jpg\n",
            "    test/\n",
            "        211_jpg.rf.48d47f24d85c8a9f111a4baf69ea315c_mask.png\n",
            "        107_jpg.rf.df9bb748e81be3e3c9fd48203429591d_mask.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCqBWJxxdtAW"
      },
      "source": [
        "## 2. Deep Residual UNet (ResUNet) Architecture\n",
        "Instead of plain convolutions, ResUNet uses residual blocks. This allows gradients to flow smoothly, preventing vanishing gradients, which is critical for thin paths like 1-pixel weld seams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f2lOI6gddtAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5848c1d8-fd01-4749-8959-3be4f49b23d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 226, Val images: 65\n"
          ]
        }
      ],
      "source": [
        "class WeldSeamDataset(Dataset):\n",
        "    def __init__(self, folder_dir, transform=None):\n",
        "        self.folder_dir = folder_dir\n",
        "        # Find only the original images (ignoring the _mask.png files)\n",
        "        self.images = [\n",
        "            f for f in os.listdir(folder_dir)\n",
        "            if f.endswith('.jpg') and not f.endswith('_mask.jpg') and not f.endswith('_mask.png')\n",
        "        ]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.folder_dir, img_name)\n",
        "\n",
        "        # The mask name is exactly the image name, but with '.jpg' replaced by '_mask.png'\n",
        "        mask_name = img_name.replace('.jpg', '_mask.png')\n",
        "        mask_path = os.path.join(self.folder_dir, mask_name)\n",
        "\n",
        "        if not os.path.exists(mask_path):\n",
        "            raise FileNotFoundError(f\"Missing mask: Expected {mask_name} in {self.folder_dir}\")\n",
        "\n",
        "        # Load Image\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load Mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        # Binarize it: any pixel > 0 becomes 1\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask.unsqueeze(0)\n",
        "\n",
        "# Augmentations remain the same\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, p=0.3),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# Use the exact paths from your output\n",
        "DATA_YAML_PATH = \"/content/dataset\"\n",
        "\n",
        "try:\n",
        "    train_dataset = WeldSeamDataset(os.path.join(DATA_YAML_PATH, 'train'), transform=train_transform)\n",
        "    val_dataset = WeldSeamDataset(os.path.join(DATA_YAML_PATH, 'valid'), transform=val_transform)\n",
        "except Exception as e:\n",
        "    print(\"Error loading datasets:\", e)\n",
        "    train_dataset, val_dataset = [], []\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2) if len(train_dataset)>0 else []\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2) if len(val_dataset)>0 else []\n",
        "\n",
        "print(f\"Train images: {len(train_dataset)}, Val images: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP6mjP_tdtAX"
      },
      "source": [
        "## 4. Loss Function\n",
        "We use BCE + Dice Loss. Dice Loss specifically helps segment thin lines gracefully, since it scores intersection over union."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdWwx0RCdtAX"
      },
      "outputs": [],
      "source": [
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, logits, targets, smooth=1.0):\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        probs_flat = probs.view(-1)\n",
        "        targets_flat = targets.view(-1)\n",
        "\n",
        "        intersection = (probs_flat * targets_flat).sum()\n",
        "        dice_loss = 1 - (2. * intersection + smooth) / (probs_flat.sum() + targets_flat.sum() + smooth)\n",
        "\n",
        "        return bce_loss + dice_loss\n",
        "\n",
        "def compute_dice_coeff(logits, targets):\n",
        "    probs = torch.sigmoid(logits) > 0.5\n",
        "    intersection = (probs & (targets > 0.5)).sum().float()\n",
        "    return (2. * intersection) / (probs.sum() + targets.sum() + 1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dwhn24udtAX"
      },
      "source": [
        "## 5. Training Loop\n",
        "Trains the ResUNet model using Mixed Precision for speed and reduced memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TJP2fm6dtAX"
      },
      "outputs": [],
      "source": [
        "model = ResUNet(in_c=3, out_c=1).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "criterion = DiceBCELoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "num_epochs = 50\n",
        "best_val_dice = 0.0\n",
        "\n",
        "train_losses, val_losses, val_dices = [], [], []\n",
        "\n",
        "if len(train_loader) > 0:\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for imgs, masks in train_loader:\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                preds = model(imgs)\n",
        "                loss = criterion(preds, masks)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, val_dice = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, masks in val_loader:\n",
        "                imgs, masks = imgs.to(device), masks.to(device)\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    preds = model(imgs)\n",
        "                    loss = criterion(preds, masks)\n",
        "                    val_loss += loss.item()\n",
        "                    val_dice += compute_dice_coeff(preds, masks).item()\n",
        "\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        val_dice = val_dice / len(val_loader)\n",
        "        val_dices.append(val_dice)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f} | Val Dice: {val_dice:.4f}\")\n",
        "\n",
        "        if val_dice > best_val_dice:\n",
        "            best_val_dice = val_dice\n",
        "            torch.save(model.state_dict(), \"best_resunet_seam.pth\")\n",
        "            print(\"--> Saved new best model\")\n",
        "\n",
        "    print(\"Training Complete. Best Val Dice:\", best_val_dice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzBTJsxYdtAX"
      },
      "source": [
        "## 6. Plotting Results and Exporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kS-OQugdtAX"
      },
      "outputs": [],
      "source": [
        "if len(train_losses) > 0:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss Curves')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(val_dices, label='Val Dice Coeff')\n",
        "    plt.legend()\n",
        "    plt.title('Validation Accuracy (Dice)')\n",
        "    plt.show()\n",
        "\n",
        "# To download the weights locally:\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('best_resunet_seam.pth')\n",
        "except:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}